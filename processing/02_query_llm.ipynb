{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from decord import VideoReader, cpu\n",
    "from llava.conversation import conv_templates\n",
    "from llava.mm_utils import tokenizer_image_token\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "# pip install git+https://github.com/LLaVA-VL/LLaVA-NeXT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从image path list中加载image\n",
    "def load_images_from_paths(paths):\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        images.append(Image.open(path).convert('RGB'))\n",
    "    return images\n",
    "\n",
    "def load_video(video_path, max_frames_num,fps=1,force_sample=False):\n",
    "    if max_frames_num == 0:\n",
    "        return np.zeros((1, 336, 336, 3))\n",
    "    vr = VideoReader(video_path, ctx=cpu(0),num_threads=1)\n",
    "    total_frame_num = len(vr)\n",
    "    video_time = total_frame_num / vr.get_avg_fps()\n",
    "    fps = round(vr.get_avg_fps()/fps)\n",
    "    frame_idx = [i for i in range(0, len(vr), fps)]\n",
    "    frame_time = [i/fps for i in frame_idx]\n",
    "    if len(frame_idx) > max_frames_num or force_sample:\n",
    "        sample_fps = max_frames_num\n",
    "        uniform_sampled_frames = np.linspace(0, total_frame_num - 1, sample_fps, dtype=int)\n",
    "        frame_idx = uniform_sampled_frames.tolist()\n",
    "        frame_time = [i/vr.get_avg_fps() for i in frame_idx]\n",
    "    frame_time = \",\".join([f\"{i:.2f}s\" for i in frame_time])\n",
    "    spare_frames = vr.get_batch(frame_idx).asnumpy()\n",
    "    # import pdb;pdb.set_trace()\n",
    "    return spare_frames,frame_time,video_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LLaVA model: lmms-lab/LLaVA-Video-7B-Qwen2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vision tower: google/siglip-so400m-patch14-384\n"
     ]
    }
   ],
   "source": [
    "# 1 加载模型\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "pretrained = \"lmms-lab/LLaVA-Video-7B-Qwen2\"\n",
    "model_name = \"llava_qwen\"\n",
    "device = \"cuda\"\n",
    "device_map = \"auto\"\n",
    "tokenizer, model, image_processor, max_length = load_pretrained_model(pretrained, None, model_name, torch_dtype=\"bfloat16\", device_map=device_map)  # Add any other thing you want to pass in llava_model_args\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 加载数据\n",
    "dataset_dir = Path(\"/root/TFVSN/dataset\")\n",
    "json_files = [\n",
    "    Path(dataset_dir,\"SumMe\",\"summe_dataset_jump.json\"),\n",
    "    Path(dataset_dir,\"SumMe\",\"summe_dataset_turn.json\"),\n",
    "    Path(dataset_dir,\"TVSum\",\"tvsum_dataset_jump.json\"),\n",
    "    Path(dataset_dir,\"TVSum\",\"tvsum_dataset_turn.json\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 执行query循环\n",
    "for dataset in json_files:\n",
    "    # 加载json\n",
    "    with open(dataset, \"r\") as f:\n",
    "        dataset = json.load(f)\n",
    "    for sample in dataset:\n",
    "        images = load_images_from_paths(sample[\"images\"])\n",
    "        video_time = sample[\"video_time\"]\n",
    "        frame_time = sample[\"frame_time\"]\n",
    "        prompt = sample[\"prompt\"]\n",
    "        prompt = \"HOW MANY FRAMES I GIVE TO YOU?\"\n",
    "        # prompt = \"Please describe the video content in detail.\"\n",
    "        images = image_processor.preprocess(images, return_tensors=\"pt\")[\"pixel_values\"].cuda().bfloat16()\n",
    "        images = [images]\n",
    "        \n",
    "        conv_template = \"qwen_1_5\"  # Make sure you use correct chat template for different models\n",
    "        time_instruciton = f\"The video lasts for {video_time:.2f} seconds, and {len(images[0])} frames are uniformly sampled from it. These frames are located at {frame_time}.Please answer the following questions related to this video.\"\n",
    "        question = DEFAULT_IMAGE_TOKEN + f\"\\n{time_instruciton}\\n{prompt}\"\n",
    "        \n",
    "        conv = copy.deepcopy(conv_templates[conv_template])\n",
    "        conv.append_message(conv.roles[0], question)\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "        \n",
    "        prompt_question = conv.get_prompt()\n",
    "        \n",
    "        input_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(device)\n",
    "        cont = model.generate(\n",
    "            input_ids,\n",
    "            images=images,\n",
    "            modalities= [\"video\"],\n",
    "            do_sample=False,\n",
    "            temperature=0,\n",
    "            max_new_tokens=500,\n",
    "        )\n",
    "        text_outputs = tokenizer.batch_decode(cont, skip_special_tokens=True)[0].strip()\n",
    "        \n",
    "        # 更新conv\n",
    "        conv.messages[1][1] = text_outputs\n",
    "        question2 = \"For the frames I gave, I first analyzed the content of each frame and then gave the importance score of the frame in the video summarization task. The output is given in the following format :\\n Frame 1: frame content\\n Score: [score] ...\"\n",
    "        conv.append_message(conv.roles[0], question2)\n",
    "        prompt2 = conv.get_prompt()\n",
    "        input_ids = tokenizer_image_token(prompt2, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(device)\n",
    "        cont = model.generate(\n",
    "            input_ids,\n",
    "            images=images,\n",
    "            modalities= [\"video\"],\n",
    "            do_sample=False,\n",
    "            temperature=0,\n",
    "            max_new_tokens=500,\n",
    "        )\n",
    "        text_outputs2 = tokenizer.batch_decode(cont, skip_special_tokens=True)[0].strip()\n",
    "        print(\"========================================\")\n",
    "        print(text_outputs2)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
